# ğŸ§Š FrozenLake â€“ Tabular Q-Learning (From Scratch)

**Author:** Omar Shaqaqi  
**Environment:** FrozenLake-v1 (Gymnasium)  
**Method:** Tabular Q-Learning  

---

## ğŸš€ Project Overview

This project implements **tabular Q-learning from scratch** to solve the classic **FrozenLake-v1** environment from Gymnasium.

The objective is to:

- Understand the agentâ€“environment interaction loop  
- Implement the Q-learning update rule manually  
- Analyze the impact of varying:
  - Learning rate (Î±)
  - Discount factor (Î³)
  - Exploration rate (Ïµ)  
- Compare performance across multiple hyperparameter combinations  

The implementation was written manually to deeply understand how Q-learning works internally.

---

## ğŸ§  Environment Description

FrozenLake is a 4Ã—4 grid-world Markov Decision Process (MDP):

- `S` â€“ Start state  
- `F` â€“ Frozen (safe state)  
- `H` â€“ Hole (terminal, reward = 0)  
- `G` â€“ Goal (terminal, reward = 1)  

Actions:
- 0 â†’ Left  
- 1 â†’ Down  
- 2 â†’ Right  
- 3 â†’ Up  

Training was performed using:

FrozenLake-v1 (is_slippery=False)

---

## ğŸ“¦ Installation

Create and activate a virtual environment (recommended with Anaconda):

conda create -n rl_qlearn python=3.10 -y  
conda activate rl_qlearn  

Install dependencies:

pip install gymnasium numpy matplotlib  

Verify installation:

python -c "import gymnasium, numpy, matplotlib; print('Installation successful')"

---

## â–¶ï¸ Running the Project

Run the main training script:

python qlearning_frozenlake.py  

The script will:

- Train the Q-learning agent  
- Evaluate the greedy policy (Ïµ = 0)  
- Generate performance plots  
- Save result images  

---

## ğŸ§ª Experiments

### ğŸ”¹ Random Policy Baseline

A purely random policy was evaluated over 100 episodes.

Result:  
Success rate â‰ˆ 0.01  

As expected, random exploration rarely reaches the goal.

---

### ğŸ”¹ Q-Learning Experiments

A 3Ã—3Ã—3 grid search was performed over:

| Parameter | Values |
|------------|---------|
| Î± (learning rate) | 0.05, 0.1, 0.2 |
| Î³ (discount factor) | 0.7, 0.9, 0.99 |
| Ïµ (exploration rate) | 0.05, 0.2, 0.5 |

Total experiments: **27**

Each configuration was evaluated using:

- Greedy policy (Ïµ = 0)  
- 100 independent evaluation episodes  

---

## ğŸ“Š Results

### 1ï¸âƒ£ Moving Average Reward (27 Experiments)

Shows convergence behavior and learning stability.

![Moving Average Reward](images/moving_avg_reward_27.png)

---

### 2ï¸âƒ£ Success Rate vs Training Episodes (27 Experiments)

Illustrates convergence speed and stability.

![Success Rate Curve](images/success_rate_27.png)

---

### ğŸ† Best Performing Configuration

Î± = 0.05  
Î³ = 0.7  
Ïµ = 0.05  

Final Success Rate = 1.00  

---

### 3ï¸âƒ£ Learned Greedy Policy (Best Model)

![Best Policy](images/policy_plot_best.png)

This policy achieves 100% success rate under deterministic dynamics.

---

## ğŸ“ˆ Observations

- Lower exploration rates (Ïµ â‰¤ 0.1) converge faster in deterministic environments.  
- Higher discount factors (0.9â€“0.99) improve long-term planning.  
- Larger learning rates may introduce instability.  
- States not visited under the optimal path may show arbitrary arrows (since their Q-values remain near zero).

---

## ğŸ—ï¸ Project Structure

â”œâ”€â”€ QLearning.py  
â”œâ”€â”€ qlearning_frozenlake.py  
â”œâ”€â”€ frozen_lake_random_policy.py  
â”œâ”€â”€ policy_plot_best.png  
â”œâ”€â”€ success_rate_27.png  
â”œâ”€â”€ moving_avg_reward_27.png  
â””â”€â”€ README.md  

---

## ğŸ§® Q-Learning Update Rule

Q(s,a) â† Q(s,a) + Î± [ r + Î³ maxâ‚â€² Q(sâ€²,aâ€²) âˆ’ Q(s,a) ]

Exploration strategy: Ïµ-greedy.

---

## ğŸ“Œ Key Takeaways

- Tabular Q-learning reliably solves deterministic FrozenLake.  
- Hyperparameter tuning significantly affects convergence speed.  
- Visualizing both reward curves and success rates provides insight into stability and learning dynamics.  
- Even small discrete environments clearly demonstrate core reinforcement learning principles.

---

## ğŸ‘¨â€ğŸ’» Author

**Omar Shaqaqi**  
MSc Robotics & Autonomous Systems  
KFUPM  

GitHub: https://github.com/OmarShaqaqi/RL-frozenLake